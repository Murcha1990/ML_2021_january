{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Семинар 8: решающие деревья"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Теоретическая часть\n",
    "\n",
    "### Предсказания в решающих деревьях\n",
    "__Вспомнить из лекции:__\n",
    "* Что такое решающее дерево? \n",
    "* Что такое предикат? Каки бывают предикаты? Какие предикаты чаще всего используются? \n",
    "* В чем отличие внутренних и листовых вершин решающего дерева?\n",
    "* Как выполнить предсказание с помощью решающего дерева в задачах многоклассовой классификации и регрессии?\n",
    "\n",
    "__На что обратить внимание__:\n",
    "* В линейной классификации мы рассматривали бинарную и многоклассовую классификацию отдельно, потому что предсказание и обучение выполняется по-разному для этих двух случаев. В решающих деревьях бинарная и многоклассовая классификация выполняются одинаково.\n",
    "* Предсказания можно сделать для любой точки признакового пространства, то есть для любого возможного объекта, а не только для объекта обучающей выборки. \n",
    "* Признаки в предикатах различных вершин могут повторяться.\n",
    "\n",
    "#### Задача 1\n",
    "Рассмотрим задачу классификации на три класса по двум признакам и следующее решающее дерево: \n",
    "\n",
    "<div>\n",
    "<img src=\"tree_class.png\" width=\"200\"/>\n",
    "</div>\n",
    "\n",
    "Какое предсказание это решающее дерево вернет для объекта $x=(7, 1.5)$? Под d1 и d2 подразумеваются первый и второй признак.\n",
    "\n",
    "\n",
    "#### Задача 2. \n",
    "Нарисуйте разделяющую поверхность для решающего дерева из предыдущей задачи.\n",
    "\n",
    "_На что обратить внимание_: в этой задаче у нас нет обучающей выборки, потому что мы предполагаем, что решающее дерево уже задано, и наша задача - как бы сделать предсказания для всех возможных объектов пространства (отсюда и возникает разделяющая поверхность). Об обучении решающего дерева по конкретной выборке мы поговорим в следующем разделе.\n",
    "\n",
    "#### Задача 3\n",
    "Рассмотрим задачу регрессии по одному признаку. Визуализируйте решающее правило    для следующего решающего дерева:\n",
    "\n",
    "<div>\n",
    "<img src=\"tree_reg.png\" width=\"200\"/>\n",
    "</div>\n",
    "\n",
    "#### Задача 4\n",
    "Приведите пример решающего дерева, которое даст нулевую ошибку в задаче классификации по двум признакам, изображенной ниже. Изобразите само решающее дерево, а также изобразите получающуюся разделяющую поверхность на рисунке. Используйте предикаты вида [j-й признак < t].\n",
    "\n",
    "<div>\n",
    "<img src=\"ideal_task.png\" width=\"200\"/>\n",
    "</div>\n",
    "\n",
    "\n",
    "### Обучение решающего дерева\n",
    "__Непрерывная и дискретная оптимизация.__ Для обучения моделей, как правило, используются методы оптимизации. В зависимости от того, по каким переменным нужно выполнять оптимизацию, выделяют два вида оптимизации: непрерывную и дискретную. Непрерывная оптимизация выполняется по вещественным числам, часто здесь используют градиентные методы. С помощью непрерывной оптимизации мы обучали линейные модели. Дискретная оптимизация выполняется по конечным множествам, для этого необходимо делать перебор по элементам множества. Делать полный перебор может быть очень долго, поэтому придумывают более эффективные алгоритмы, например один из самых простых - жадный алгоритм. С помощью жадной дискретной оптимизации мы будем обучать решающие деревья. В решающих деревьях нужно выбрать одно из всех возможных разбиений признакового пространства на области. Несмотря на то, что всего разбиений бесконечное число, разбиений, приводящих к разным предсказаниям на обучающей выборке, конечное число - между ними и нужно выбрать одно.\n",
    "\n",
    "__Вспомнить из лекции:__\n",
    "* Рекурсивный алгоритм построения решающего дерева.\n",
    "* Какие вы знаете критерии останова при построении решающего дерева (когда выборку не нужно делить, а нужно сделать лист)?\n",
    "* Как выбрать, какое предсказание делать в листе для задачи регрессии и задачи классификации?\n",
    "\n",
    "__Критерии информативности: интуиция.__ \n",
    "Когда мы строим одну вершину решающего дерева, мы разбиваем выборку на две подвыборки: одна подвыборка будет использоваться при построении левого поддерева, вторая - при построении правого поддерева. \n",
    "Какую цель мы преследуем, когда разбиваем выборку на две подвыборки? Каких свойств мы хотим от получающихся подвыборок? Можно придумать разные ответы, но на практике хорошо работает следующее желаемое свойство: мы хотим, чтобы ответы в обеих подвыборках были как можно менее вариативны. В идеальной ситуации в каждой подвыборке у всех объектов одинаковые ответы, и дальше строить дерево не нужно (каждая подвыборка станет листом). На практике так обычно не получается, иначе это бы уже не было задачей машинного обучения. Однако мы можем постепенно уменьшать вариативность ответов в подвыборках. Итак, зафиксируем следующую постановку задачи: у нас есть вектор правильных ответов $Y_v$ (ответы на всех объектах, попавших в вершину $v$), и мы хотим измерить вариативность этих ответов.\n",
    "\n",
    "Как измерить вариативность в задаче регрессии? Для этого можно использовать среднеквадратичное отклонение:\n",
    "$$H(Y_v) = \\sum_{i=1}^{|Y_v|} (Y_{v, i} - \\bar Y_{v})^2,$$\n",
    "где $\\bar Y_{v}$ - среднее вектора $Y_{v}$.\n",
    "Можно выбирать аналогичные меры разброса в зависимости от функции потерь, используемой в задаче. Например, при использовании MAE можно заменить квадрат на модуль, а среднее - на медиану.\n",
    "\n",
    "Как измерить вариативность в задаче классификации? Здесь обычно подходят следующим образом: на основе вектора $Y_v$, состоящего из меток классов $1 \\dots K$, вычисляют доли каждого класса: $(p_1, \\dots, p_K)$. Если все элементы вектора одинаковые (вариативность наименьшая), то среди $p_k$ будет одна 1, оостальные значения 0 (назовем это случай а). При наименьшей вариативности вектора $Y_v$ все $p_k \\approx \\frac 1 k$ (случай б). Осталось придумать критерий, зависящий от $(p_1, \\dots, p_K)$, который минимален в случае а и максимален в случае б. Таким критерием является, например, энтропийный критерий:\n",
    "$$H(Y_v) = H(p_1, \\dots, p_K) = - \\sum_{k=1}^K p_k \\log p_k$$\n",
    "или критерий Джини:\n",
    "$$H(Y_v) = H(p_1, \\dots, p_K) = \\sum_{k=1}^K p_k (1-p_k).$$\n",
    "\n",
    "В итоге, мы можем измерить вариативность подвыборок, из которых будет строиться левое и правое поддерево. Чтобы построить вершину $v$, мы будем перебирать все возможные признаки $j$ и все возможные пороги $t$ (всего $\\ell d$ вариантов, $\\ell$ - число объектов, $d$ - число признаков), для каждой пары $(j, t)$ мы получим разбиение выборки на две части, им соответствуют векторы правильных ответов $Y_\\ell$ и $Y_r$. Далее нам нужно сравнить все разбиения и выбрать лучшее. Сравнивать разбиения удобно, когда есть один критерий;  у нас же пока два критерия. Скомбинируем вариативность обеих выборок в одном критерие:\n",
    "$$Q(Y_v, j, t) = \\frac {|Y_\\ell|}{|Y_v|} H(Y_\\ell) \n",
    "+ \\frac {|Y_r|}{|Y_v|} H(Y_r) \\rightarrow \\min_{j, \\,t}$$\n",
    "\n",
    "Веса $\\frac {|Y_\\ell|}{|Y_v|}$ и $\\frac {|Y_r|}{|Y_v|}$ вводятся для того, чтобы не поощрять отделение одного объекта в отдельную вершину: для таких подвыборок вариативность вектора правильных ответов низкая, и без перевзвешивания алгоритм всегда выбирал бы подобные разбиения. А это нежелательно, потому что вершины из одного объекта позволяют запоминать ответы, что ведет к переобучению. \n",
    "\n",
    "Итоговый алгоритм построения вершины: перебрать все варианты $(j, t)$, для каждого посчитать критерий $Q(Y_v, j, t)$, выбрать пару с наибольшим значением критерия.\n",
    "\n",
    "#### Задача 5\n",
    "Предположим, мы решаем задачу классификации на три класса по четырем признакам и строим решающее дерево. При построении вершины $v$ мы имеем выборку из семи объектов:\n",
    "\n",
    "| 1-й признак | 2-й признак | 3-й признак | 4-й признак | класс |\n",
    "|-------------|-------------|-------------|-------------|-------|\n",
    "| 5           | 8           | 8           | 2.5         | 2     |\n",
    "| 3           | 3           | 7           | 7.7         | 1     |\n",
    "| 6           | 7.7         | 1           | 1.1         | 3     |\n",
    "| 3.3         | 1           | 2           | 1.2         | 1     |\n",
    "| 24          | 3.9         | 5           | 3.9         | 1     |\n",
    "| 12          | 10          | 10.1        | 8           | 2     |\n",
    "| 1           | 2           | 2           | 9.1         | 1     |\n",
    "\n",
    "Какой из предикатов лучше: [1-й признак < 6] или [3-й признак < 5] по критерию Джини?\n",
    "\n",
    "\n",
    "#### Вопрос: Что является параметрами и гиперпараметрами решающих деревьев?\n",
    "\n",
    "__Решение:__\n",
    "\n",
    "Когда мы говорим о семействе алгоритмов машинного обучения, мы задаем алгоритм, как по входному объекту сделать предсказание. В этом алгоритме обычно участвуют две группы величин: признаки объекта (даны) и параметры (неизвестные величины). В ходе обучения мы настраиваем параметры по обучающей выборке, чтобы получить конкретный алгоритм предсказания. В алгоритмах предсказания и обучения могут также присутствовать величины, которые мы должны задать сами, не по данным; их называют гиперпараметры.\n",
    "\n",
    "В решающих деревьях мы задали алгоритм предсказания как проход вниз по дереву, в каждой вершине которого находится предикат или предсказание. Вот эти придикаты и предсказания и есть параметры решающего дерева (они настраиваются по обучающей выборке). Для самого популярного вида предикатов [j-й признак < t] параметрами являются признаки и пороги в каждой внутренней вершине, а также константные предсказания в листовых вершинах.\n",
    "\n",
    "Гиперпараметрами решающего дерева являются все детали обучения: критерии останова, критерий информативности и т. д. Используя разные критерии останова и криерии информативности, мы получим разные решающие деревья. Какие лучше, нужно выбирать по валидационной выборке или кросс-валидации.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рассмотрим модельную задачу регрессии. Объектами будут являться точки на плоскости (т.е. каждый объект описывается 2 признаками), целевая переменная — расстояние от объекта до точки (0, 0)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Напишем вспомогательную функцию, которая будет возвращать решетку для дальнейшей красивой визуализации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_grid(data):\n",
    "    x_min, x_max = data[:, 0].min() - 1, data[:, 0].max() + 1\n",
    "    y_min, y_max = data[:, 1].min() - 1, data[:, 1].max() + 1\n",
    "    return np.meshgrid(np.arange(x_min, x_max, 0.01),\n",
    "                         np.arange(y_min, y_max, 0.01))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сгенерируем выборку"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_x = np.random.normal(size=(100, 2))\n",
    "data_y = (data_x[:, 0] ** 2 + data_x[:, 1] ** 2) ** 0.5\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.scatter(data_x[:, 0], data_x[:, 1], c=data_y, s=100, cmap='spring')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучим дерево на сгенерированных данных и предскажем ответы для каждой точки решетки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "clf = DecisionTreeRegressor()\n",
    "clf.fit(data_x, data_y)\n",
    "\n",
    "xx, yy = get_grid(data_x)\n",
    "print(np.c_[xx.ravel(), yy.ravel()])\n",
    "\n",
    "predicted = clf.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.pcolormesh(xx, yy, predicted, cmap='spring')\n",
    "plt.scatter(data_x[:, 0], data_x[:, 1], c=data_y, s=100, cmap='spring')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание\n",
    "\n",
    "Сейчас мы сгенерировали 100 точек из нормального распределения и обучили решающее дерево на них. Сгенерируйте 300 точек из нормального распределения, обучите на них дерево и выведите на экран результат (как на картинке выше).\n",
    "\n",
    "Сгенерированные точки и расстояние до точек сохраните в массивы data_x300, data_y300, для обучения и предсказания используйте эти массивы.\n",
    "\n",
    "Улучшилось ли предсказание алгоритма на решётке? (т.е. стала ли раскраска всей плоскости более правильной?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вернёмся к исходным данным (100 точек).\n",
    "\n",
    "Посмотрим как будут выглядеть разделяющая поверхность в зависимости от \n",
    "- минимального количества объектов в листе\n",
    "- максимальной глубины дерева"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18, 18))\n",
    "for i, max_depth in enumerate([1, 2, 4, 6]):\n",
    "    for j, min_samples_leaf in enumerate([1, 5, 10, 15]):\n",
    "        clf = DecisionTreeRegressor(max_depth=max_depth, min_samples_leaf=min_samples_leaf)\n",
    "        clf.fit(data_x, data_y)\n",
    "        xx, yy = get_grid(data_x)\n",
    "        predicted = clf.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)\n",
    "        \n",
    "        plt.subplot2grid((4, 4), (i, j))\n",
    "        plt.pcolormesh(xx, yy, predicted, cmap='spring')\n",
    "        plt.scatter(data_x[:, 0], data_x[:, 1], c=data_y, s=30, cmap='spring')\n",
    "        plt.title('max_depth=' + str(max_depth) + ', min_samples_leaf: ' + str(min_samples_leaf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Как влияет увеличение максимальной глубины и/или уменьшение минимального количества объектов выборки в листе на качество на обучающей выборке? на переобучение?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Неустойчивость решающих деревьев\n",
    "\n",
    "Решающие деревья — это алгоритмы, неустойчивые к изменениям обучающей выборки, т.е. при малейших её изменениях итоговый классификатор может радикально измениться.\n",
    "Посмотрим, как будет меняться структура дерева при обучении на разных 90%-х подвыборках."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 6))\n",
    "for i in range(3):\n",
    "    clf = DecisionTreeRegressor(random_state=42)\n",
    "\n",
    "    indices = np.random.randint(data_x.shape[0], size=int(data_x.shape[0] * 0.9))\n",
    "    clf.fit(data_x[indices], data_y[indices])\n",
    "    xx, yy = get_grid(data_x)\n",
    "    predicted = clf.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)\n",
    "\n",
    "    plt.subplot2grid((1, 3), (0, i))\n",
    "    plt.pcolormesh(xx, yy, predicted, cmap='winter')\n",
    "    plt.scatter(data_x[:, 0], data_x[:, 1], c=data_y, s=30, cmap='winter')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Подбор параметров"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим на качество дерева в зависимости от параметров на одном из стандартных наборов данных - Бостонском датасете."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "\n",
    "data = load_boston()\n",
    "print(data.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- будем оценивать качество алгоритма по кросс-валидации"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можем зафиксировать разбиение на фолды, чтобы затем каждый раз использовать одно и то же разбиение при кросс-валидации, это полезно при сравнении алгоритмов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "cv = KFold(X.shape[0], shuffle=True, random_state=241)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выведите качество DecisionTreeRegressor, обученного на данных X, y по кросс-валидации. В функции cross_val_score в качестве cv поставьте cv=cv, в качестве метрики - 'neq_mean_squared_error'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание\n",
    "\n",
    "Метрика MSЕ имеет не ограничена сверху. Поэтому для оценки качества алгоритма можно также пользоваться метрикой R2 (коэффициент детерминации), так как он не превышает 1 (и чем ближе к 1, тем лучше).\n",
    "\n",
    "Выведите на экран значение R2 алгоритма ('r2')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для сравнения качества модели при различных наборах параметров или для сравнения моделей на одном датасете можно использовать, как и раньше, MSE.\n",
    "\n",
    "Будем подбирать параметры решающего дерева по сетке с целью увеличить качество алгоритма. Будем подбирать значения max_features и max_depth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import SCORERS\n",
    "SCORERS.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подберите по кросс-валидации оптимальные значения max_features и max_depth. В функции GridSearchCV в качестве cv поставьте заранее фиксированное разбиение (cv=cv), метрику качества используйте scoring='neq_mean_squared_error'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params={'max_features': [None, 'log2', 'sqrt'], \n",
    "        'max_depth': [2, 4, 6, 8, 10, 20, 50]},\n",
    "\n",
    "gs = GridSearchCV(DecisionTreeRegressor(), params, cv=3, n_jobs=-1)\n",
    "\n",
    "gs.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выведем на экран средние значения и стандартные отклонения, полученные при GridSearch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "means = gs.cv_results_['mean_test_score']\n",
    "stds = gs.cv_results_['std_test_score']\n",
    "for mean, std, params in zip(means, stds, gs.cv_results_['params']):\n",
    "    print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "            % (mean, std * 2, params))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задание\n",
    "\n",
    "Теперь попробуем одновременно подбирать значения max_features, max_depth и min_samples_leaf. Ищите min_samples_leaf в диапазоне от 1 до 20 с шагом 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как в данной задаче зависит качество алгоритма от количества параметров, которые мы оптимизируем?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
