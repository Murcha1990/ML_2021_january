{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA aka Latent Dirichlet Allocation (не путать с LDA = Linear Discriminant Analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\elena\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\elena\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#запустить эту ячейку до начала занятия\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import *\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import  CountVectorizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.decomposition import PCA,LatentDirichletAllocation\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Тематическое моделирование (topic modelling)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Тематическое моделирование - это присваивание темы (topic) каждому документу. Каждая тема представлена определенными словами.\n",
    "\n",
    "Рассмотрим пример:\n",
    "\n",
    "У нас есть два топика: топик 1 и топик 2. Топик1 представлен словами \"apple, banana, mange\",\n",
    "топик2 - словами \"tennis, cricket, hockey\". Можем предположить, что в топике1 речь идет о фруктах, а в топике2 - о спорте. Затем каждому новому документу мы присваиваем одну из этих тем (топик1 или топик2).\n",
    "\n",
    "Другой пример: предположим, у нас есть 6 документов\n",
    "\n",
    "apple banana\n",
    "apple orange\n",
    "banana orange\n",
    "tiger cat\n",
    "tiger dog\n",
    "cat dog\n",
    "\n",
    "Что будет происходить с тематическим моделированием, если мы захотим извлечь две темы (два топика) из этих документов?\n",
    "Мы получим два распределения: распределение тема-слово (topic-word) и распределение документ-тема (doc-topic).\n",
    "\n",
    "Идеальное распределение документ-слово в данном примере будет таким:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![How](df1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Идеальное распределение документ-тема будет таким:\n",
    "\n",
    "![How](df2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Предположим, что у нас есть новый документ \"cat dog apple\", тогда его представление по темам должно быть следующим:\n",
    "\n",
    "Topic1: 0.33\n",
    "\n",
    "Topic2: 0.66\n",
    "\n",
    "LDA широко применяется в таких задачах. Его использование для тематического моделирования продемонстрировано ниже. \n",
    "\n",
    "Мы подаем на вход LDA число тем (topics), которые хотим выделить в корпусе. \n",
    "\n",
    "Но сначала необходимо векторизовать слова (будем использовать подход - мешок слов), поэтому взаимосвязь между словами в текстах при таком подходе исчезнет."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\elena\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\elena\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer() #For words Lemmatization\n",
    "stemmer = PorterStemmer()  #For stemming words\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TokenizeText(text):\n",
    "    ''' \n",
    "     Tokenizes text by removing various stopwords and lemmatizing them\n",
    "    '''\n",
    "    text=re.sub('[^A-Za-z0-9\\s]+', '', text)\n",
    "    word_list=word_tokenize(text)\n",
    "    word_list_final=[]\n",
    "    \n",
    "    for word in word_list:\n",
    "        if word not in stop_words:\n",
    "            word_list_final.append(lemmatizer.lemmatize(word))\n",
    "    return word_list_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gettopicwords(topics, cv, n_words=10):\n",
    "    '''\n",
    "        Print top n_words for each topic.\n",
    "        cv=Countvectorizer\n",
    "    '''\n",
    "    for i, topic in enumerate(topics):\n",
    "        top_words_array = np.array(cv.get_feature_names())[np.argsort(topic)[::-1][:n_words]]\n",
    "        print(\"For  topic {} it's top {} words are \".format(str(i),str(n_words)))\n",
    "             \n",
    "        combined_sentence=\"\"\n",
    "        for word in top_words_array:\n",
    "            combined_sentence+=word+\" \"\n",
    "        print(combined_sentence)\n",
    "#        print(\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('million-headlines.zip',usecols=[1])\n",
    "df = df.iloc[:10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\elena\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data link:\n",
    "\n",
    "https://www.kaggle.com/therohk/million-headlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headline_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aba decides against community broadcasting lic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>act fire witnesses must be aware of defamation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>a g calls for infrastructure protection summit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>air nz staff in aust strike for pay rise</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>air nz strike to affect australian travellers</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       headline_text\n",
       "0  aba decides against community broadcasting lic...\n",
       "1     act fire witnesses must be aware of defamation\n",
       "2     a g calls for infrastructure protection summit\n",
       "3           air nz staff in aust strike for pay rise\n",
       "4      air nz strike to affect australian travellers"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(df))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 3.59 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "num_features = 100000\n",
    "# cv=CountVectorizer(min_df=0.01,max_df=0.97,tokenizer=TokenizeText,max_features=num_features)\n",
    "cv = CountVectorizer(tokenizer=TokenizeText, max_features=num_features)\n",
    "transformed_data = cv.fit_transform(df['headline_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 27.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "no_topics=10  ## We can change this, hyperparameter\n",
    "lda = LatentDirichletAllocation(n_components=no_topics, max_iter=5, learning_method='online', \\\n",
    "                                learning_offset=50.,random_state=0, n_jobs=-1).fit(transformed_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lda.components_ - это таблица тема-слово, она показывает, какими словами представлена каждая тема."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For  topic 0 it's top 10 words are \n",
      "crash sars woman new lead final car dead clash strike \n",
      "For  topic 1 it's top 10 words are \n",
      "iraqi force plan three missing case missile open air denies \n",
      "For  topic 2 it's top 10 words are \n",
      "u iraq war say troop anti get fire australia wa \n",
      "For  topic 3 it's top 10 words are \n",
      "back killed set nsw coalition home minister year election korea \n",
      "For  topic 4 it's top 10 words are \n",
      "man police face court vic take two charge charged coast \n",
      "For  topic 5 it's top 10 words are \n",
      "world saddam pm dy cup melbourne former stay power blue \n",
      "For  topic 6 it's top 10 words are \n",
      "win say protest marine found union make battle probe accident \n",
      "For  topic 7 it's top 10 words are \n",
      "baghdad may hospital hit group support concern seek ban inquiry \n",
      "For  topic 8 it's top 10 words are \n",
      "council govt death claim police plan qld new water hope \n",
      "For  topic 9 it's top 10 words are \n",
      "call report attack begin bridge near work appeal rail put \n"
     ]
    }
   ],
   "source": [
    "gettopicwords(lda.components_,cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Присваивание темы документу\n",
    "\n",
    "Можно заметить, что каждый документ содержит комбинацию тем. Посмотрим на темы первых десяти документов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = df['headline_text'][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "for doc in docs:\n",
    "    data.append(lda.transform(cv.transform([doc])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['topic'+str(i) for i in range(1,11)]\n",
    "doc_topic_df = pd.DataFrame(columns=cols, data=np.array(data).reshape((10,10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_topic_df['major_topic'] = doc_topic_df.idxmax(axis=1)\n",
    "doc_topic_df['raw_doc'] = docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic1</th>\n",
       "      <th>topic2</th>\n",
       "      <th>topic3</th>\n",
       "      <th>topic4</th>\n",
       "      <th>topic5</th>\n",
       "      <th>topic6</th>\n",
       "      <th>topic7</th>\n",
       "      <th>topic8</th>\n",
       "      <th>topic9</th>\n",
       "      <th>topic10</th>\n",
       "      <th>major_topic</th>\n",
       "      <th>raw_doc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.850000</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>topic4</td>\n",
       "      <td>aba decides against community broadcasting lic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.014286</td>\n",
       "      <td>0.014286</td>\n",
       "      <td>0.871428</td>\n",
       "      <td>0.014286</td>\n",
       "      <td>0.014286</td>\n",
       "      <td>0.014286</td>\n",
       "      <td>0.014286</td>\n",
       "      <td>0.014286</td>\n",
       "      <td>0.014286</td>\n",
       "      <td>0.014286</td>\n",
       "      <td>topic3</td>\n",
       "      <td>act fire witnesses must be aware of defamation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.683333</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.183334</td>\n",
       "      <td>topic1</td>\n",
       "      <td>a g calls for infrastructure protection summit</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     topic1    topic2    topic3    topic4    topic5    topic6    topic7  \\\n",
       "0  0.016667  0.016667  0.016667  0.850000  0.016667  0.016667  0.016667   \n",
       "1  0.014286  0.014286  0.871428  0.014286  0.014286  0.014286  0.014286   \n",
       "2  0.683333  0.016667  0.016667  0.016667  0.016667  0.016667  0.016667   \n",
       "\n",
       "     topic8    topic9   topic10 major_topic  \\\n",
       "0  0.016667  0.016667  0.016667      topic4   \n",
       "1  0.014286  0.014286  0.014286      topic3   \n",
       "2  0.016667  0.016667  0.183334      topic1   \n",
       "\n",
       "                                             raw_doc  \n",
       "0  aba decides against community broadcasting lic...  \n",
       "1     act fire witnesses must be aware of defamation  \n",
       "2     a g calls for infrastructure protection summit  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_topic_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы увидели, как LDA может быть использован для тематического моделирования. Такой подход также может быть применен для кластеризации документов, основанной на группировке по темам."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ссылки\n",
    "\n",
    "https://www.analyticsvidhya.com/blog/2016/08/beginners-guide-to-topic-modeling-in-python/\n",
    "\n",
    "https://sebastianraschka.com/faq/docs/lda-vs-pca.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
